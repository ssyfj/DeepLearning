{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讲解看01神经网络实现（含反向传播和梯度下降）.ipynb和https://www.zybuluo.com/hanbingtao/note/476663\n",
    "  \n",
    "这里将前面函数整合为类进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一：实现激活类SigmoidActivator\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidActivator(object):\n",
    "    #实现sigmoid函数\n",
    "    def forward(self,Z): #之所以用forward和backward,是为了兼容多个不同的激活项对象\n",
    "        return 1/(1+np.exp(-Z))\n",
    "    \n",
    "    #下面进行反向传播\n",
    "    #各种函数求导：平方损失、交叉熵、多累分类https://zhuanlan.zhihu.com/p/99923080\n",
    "    #sigmoid求导\n",
    "    def backward(self,output): #注意：这里我们在前向传播中获取了输出值，不需要再次计算\n",
    "        return output*(1-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二：实现全连接类\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullConnectedLayer(object):\n",
    "    def __init__(self,input_size,output_size,activator):\n",
    "        \"\"\"\n",
    "        input_size:输入向量维度(激活单元数量)\n",
    "        output_size：输出层向量维度(激活单元数量)\n",
    "        activator：激活函数\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activator = activator\n",
    "        \n",
    "        #根据上面构造权值信息\n",
    "        self.W = np.random.normal(-0.1,0.1,(output_size,input_size))\n",
    "        #self.W = np.ones((output_size,input_size))\n",
    "        self.b = np.zeros((output_size,1))\n",
    "        \n",
    "        #输出向量大小\n",
    "        self.output = np.zeros((output_size,1))\n",
    "    \n",
    "    #注意下面前向传播和反向传播结合全链接层，前向传播获取输出层的激活项值，所以默认的我们的input也应该是上一个全链接层的输出激活项值\n",
    "\n",
    "    def forward(self,input_array): #实现前向传播\n",
    "        self.input = input_array\n",
    "        self.output = self.activator.forward(self.input@self.W.T + self.b.T)\n",
    "        \n",
    "    def backward(self,delta_array): #实现反向传播\n",
    "        \"\"\"\n",
    "        delta_array是输出层之后的误差项\n",
    "        \"\"\"\n",
    "        #注意：在执行backward之前，我们一定确保执行了forward,获取了input和output\n",
    "        #print(delta_array.shape,self.W.shape,self.output.shape)\n",
    "        self.delta =  self.activator.backward(self.input)*(delta_array@self.W)#我们求得delta是输入层的误差项\n",
    "        #上面求解的self.delta是为上一个全链接层做准备，我们求解梯度，至于本层的delta有关\n",
    "        #print(delta_array.shape,self.output.shape)\n",
    "        #print(delta_array.T.shape,self.input.shape)\n",
    "        #print(\"----------\")\n",
    "        self.W_grad = delta_array.T@self.input\n",
    "        self.b_grad = delta_array.T\n",
    "        \n",
    "    def update(self,learning_rate):\n",
    "        self.W += learning_rate*self.W_grad\n",
    "        #print(self.b.shape,self.b_grad.shape)\n",
    "        #print(0000000000)\n",
    "        self.b += learning_rate*self.b_grad\n",
    "        \n",
    "    def dump(self): #输出每一次的W和b信息\n",
    "        print(\"W and b:\")\n",
    "        print(self.W,self.b)\n",
    "        print(\"============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三：神经网络构建类\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self,layers):\n",
    "        self.layers = []\n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(FullConnectedLayer(layers[i],layers[i+1],SigmoidActivator()))\n",
    "    \n",
    "    def predict(self,X): #预测函数（实际就是对整个网络实现了一次前向传播）\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            layer.forward(output)\n",
    "            output = layer.output\n",
    "        return output\n",
    "    \n",
    "    def calc_gradient(self,label,y_pred):\n",
    "        #先求出最后输出层的delta项\n",
    "        delta = self.layers[-1].activator.backward(y_pred)*(y_pred-label)\n",
    "        #遍历每一层，获取每一层的delta保存在原有数据结构中\n",
    "        for layer in self.layers[::-1]:\n",
    "            layer.backward(delta) #backward会将数据保存在layer.delta中，并且获取了每一层的梯度信息\n",
    "            delta = layer.delta\n",
    "            \n",
    "    def update_weight(self,learning_rate):\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "    \n",
    "    def train_one_sample(self,label,sample,learning_rate): #训练一个样本\n",
    "        y_pred = self.predict(sample)\n",
    "        print(self.loss(y_pred,label))\n",
    "        self.calc_gradient(label,y_pred)\n",
    "        self.update_weight(learning_rate)\n",
    "    \n",
    "    def train(self,labels,data_set,learning_rate,epoch): #训练函数\n",
    "        for i in range(epoch):\n",
    "            print(\"==============================\")\n",
    "            for d in range(len(data_set)):\n",
    "                self.train_one_sample(np.array([labels[d]]),np.array([data_set[d]]),learning_rate)\n",
    "                \n",
    "    def dump(self):\n",
    "        for layer in self.layers:\n",
    "            layer.dump()\n",
    "            \n",
    "    def loss(self,output,label):\n",
    "        return np.sum(1/2*(np.power(output-label,2)))\n",
    "    \n",
    "    def gradient_check(self,sample,label): #梯度检测\n",
    "        #反向传播下的梯度获取\n",
    "        y_pred = self.predict(sample)\n",
    "        #print(y_pred,label)\n",
    "        #print(y_pred.shape,label.shape)\n",
    "        self.calc_gradient(label,y_pred)\n",
    "        \n",
    "        #求导情况下的导数\n",
    "        epsilon = 1e-4\n",
    "        for fc in self.layers:\n",
    "            for i in range(fc.W.shape[0]):\n",
    "                for j in range(fc.W.shape[1]):\n",
    "                    fc.W[i,j] += epsilon\n",
    "                    output = self.predict(sample)\n",
    "                    err1 = self.loss(output,label)\n",
    "                    \n",
    "                    fc.W[i,j] -= 2*epsilon\n",
    "                    output = self.predict(sample)\n",
    "                    err2 = self.loss(output,label)\n",
    "                    \n",
    "                    expect_grad = (err1 - err2)/(2*epsilon)\n",
    "                    \n",
    "                    #还原权值\n",
    "                    fc.W[i,j] += epsilon\n",
    "                    print(\"Weights(%d %d) expected:%f actual:%f expect - actual = :%f\"%(i,j,expect_grad,fc.W_grad[i,j],expect_grad-fc.W_grad[i,j]))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights(0 0) expected:-0.015661 actual:-0.015661 expect - actual = :0.000000\n",
      "Weights(0 1) expected:-0.031323 actual:-0.031323 expect - actual = :0.000000\n",
      "Weights(0 2) expected:-0.015661 actual:-0.015661 expect - actual = :0.000000\n",
      "Weights(0 3) expected:-0.015661 actual:-0.015661 expect - actual = :0.000000\n",
      "Weights(0 4) expected:-0.015661 actual:-0.015661 expect - actual = :0.000000\n",
      "Weights(0 5) expected:-0.015661 actual:-0.015661 expect - actual = :0.000000\n",
      "Weights(0 6) expected:-0.015661 actual:-0.015661 expect - actual = :0.000000\n",
      "Weights(0 7) expected:0.000000 actual:0.000000 expect - actual = :0.000000\n",
      "Weights(1 0) expected:-0.025671 actual:-0.025671 expect - actual = :0.000000\n",
      "Weights(1 1) expected:-0.051342 actual:-0.051342 expect - actual = :0.000000\n",
      "Weights(1 2) expected:-0.025671 actual:-0.025671 expect - actual = :0.000000\n",
      "Weights(1 3) expected:-0.025671 actual:-0.025671 expect - actual = :0.000000\n",
      "Weights(1 4) expected:-0.025671 actual:-0.025671 expect - actual = :0.000000\n",
      "Weights(1 5) expected:-0.025671 actual:-0.025671 expect - actual = :0.000000\n",
      "Weights(1 6) expected:-0.025671 actual:-0.025671 expect - actual = :0.000000\n",
      "Weights(1 7) expected:0.000000 actual:0.000000 expect - actual = :0.000000\n",
      "Weights(2 0) expected:-0.028786 actual:-0.028786 expect - actual = :0.000000\n",
      "Weights(2 1) expected:-0.057573 actual:-0.057573 expect - actual = :0.000000\n",
      "Weights(2 2) expected:-0.028786 actual:-0.028786 expect - actual = :0.000000\n",
      "Weights(2 3) expected:-0.028786 actual:-0.028786 expect - actual = :0.000000\n",
      "Weights(2 4) expected:-0.028786 actual:-0.028786 expect - actual = :0.000000\n",
      "Weights(2 5) expected:-0.028786 actual:-0.028786 expect - actual = :0.000000\n",
      "Weights(2 6) expected:-0.028786 actual:-0.028786 expect - actual = :0.000000\n",
      "Weights(2 7) expected:0.000000 actual:0.000000 expect - actual = :0.000000\n",
      "Weights(0 0) expected:-0.044225 actual:-0.044225 expect - actual = :0.000000\n",
      "Weights(0 1) expected:-0.054320 actual:-0.054320 expect - actual = :0.000000\n",
      "Weights(0 2) expected:-0.046452 actual:-0.046452 expect - actual = :0.000000\n",
      "Weights(1 0) expected:0.037776 actual:0.037776 expect - actual = :-0.000000\n",
      "Weights(1 1) expected:0.046399 actual:0.046399 expect - actual = :-0.000000\n",
      "Weights(1 2) expected:0.039678 actual:0.039678 expect - actual = :-0.000000\n",
      "Weights(2 0) expected:0.035948 actual:0.035948 expect - actual = :-0.000000\n",
      "Weights(2 1) expected:0.044154 actual:0.044154 expect - actual = :-0.000000\n",
      "Weights(2 2) expected:0.037759 actual:0.037759 expect - actual = :-0.000000\n",
      "Weights(3 0) expected:0.036064 actual:0.036064 expect - actual = :-0.000000\n",
      "Weights(3 1) expected:0.044296 actual:0.044296 expect - actual = :-0.000000\n",
      "Weights(3 2) expected:0.037880 actual:0.037880 expect - actual = :-0.000000\n",
      "Weights(4 0) expected:0.037182 actual:0.037182 expect - actual = :-0.000000\n",
      "Weights(4 1) expected:0.045669 actual:0.045669 expect - actual = :-0.000000\n",
      "Weights(4 2) expected:0.039055 actual:0.039055 expect - actual = :-0.000000\n",
      "Weights(5 0) expected:0.037954 actual:0.037954 expect - actual = :-0.000000\n",
      "Weights(5 1) expected:0.046617 actual:0.046617 expect - actual = :-0.000000\n",
      "Weights(5 2) expected:0.039865 actual:0.039865 expect - actual = :-0.000000\n",
      "Weights(6 0) expected:0.037545 actual:0.037545 expect - actual = :-0.000000\n",
      "Weights(6 1) expected:0.046114 actual:0.046114 expect - actual = :-0.000000\n",
      "Weights(6 2) expected:0.039435 actual:0.039435 expect - actual = :-0.000000\n",
      "Weights(7 0) expected:0.036162 actual:0.036162 expect - actual = :-0.000000\n",
      "Weights(7 1) expected:0.044416 actual:0.044416 expect - actual = :-0.000000\n",
      "Weights(7 2) expected:0.037982 actual:0.037982 expect - actual = :-0.000000\n"
     ]
    }
   ],
   "source": [
    "data_set = np.array([[1,2,1,1,1,1,1,0]])\n",
    "labels = np.array([[1,0,0,0,0,0,0,0]])\n",
    "net = Network([8, 3, 8])\n",
    "\n",
    "net.gradient_check(data_set,labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
